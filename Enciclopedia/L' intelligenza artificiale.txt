L' intelligenza artificiale
L'intelligenza artificiale (in sigla IA), nel suo significato più ampio, è la capacità (o il tentativo) di un sistema artificiale (tipicamente un sistema informatico) di simulare l'intelligenza umana attraverso l'ottimizzazione di funzioni matematiche. In particolare, si tratta di un settore specifico della ricerca scientifica nelle scienze informatiche che studia e sviluppa programmi in grado di svolgere predizioni a partire da un insieme di dati non impiegato durante la fase di allenamento di questi ultimi. In breve, la realizzazione di un'intelligenza artificiale inizia con la raccolta di dati, alla quale segue una fase di progettazione e programmazione di un modello (tipicamente costituito da blocchi composti da funzioni matematiche aventi parametri apprendibili) basato su una architettura compatibile ai tipi di predizione che si intende ottenere (riconoscimento facciale, generazione di testo, segmentazione di immagini, rilevamento di oggetti ecc.). L'allenamento del modello avviene calcolando una funzione di costo (ad esempio Cross-entropy loss, MSE loss ecc.) tra le predizioni e le annotazioni (caso dell'allenamento supervisionato), il cui valore viene utilizzato per aggiornare i parametri del modello tipicamente sfruttando la discesa del gradiente.

L'etica dell'intelligenza artificiale è una disciplina dibattuta tra scienziati e filosofi che manifesta numerosi aspetti sia teorici sia pratici.[1] Stephen Hawking nel 2014 ha messo in guardia riguardo ai pericoli dell'intelligenza artificiale, considerandola una minaccia per la sopravvivenza dell'umanità.[2]
La norma ISO/IEC 42001:2023 Information technology - Artificial intelligence Management System (AIMS) definisce l'intelligenza artificiale come la capacità di un sistema di mostrare capacità umane quali il ragionamento, l'apprendimento, la pianificazione e la creatività.

Tradizione di ricerca

Molteplici furono i passi che portarono alla nascita di questa disciplina. Il primo, sia a livello di importanza sia di ordine cronologico, è l'avvento dei calcolatori e il continuo interesse rivolto a essi. Già nel 1623, grazie a Wilhelm Schickard, si arrivò a creare macchine in grado di effettuare calcoli matematici con numeri fino a sei cifre, anche se non in maniera autonoma. Nel 1642 Blaise Pascal costruì una macchina in grado di fare operazioni utilizzando il riporto automatico, mentre nel 1674 Gottfried Wilhelm von Leibniz creò una macchina in grado di effettuare la somma, la differenza e la moltiplicazione in maniera ricorsiva. Tra il 1834 e il 1837 Charles Babbage lavorò al modello di una macchina chiamata macchina analitica, le cui caratteristiche anticiparono in parte quelle dei moderni calcolatori. Nel ventesimo secolo l'attenzione sui computer ritornò ad accendersi: nel 1937, ad esempio, Claude Shannon, all'università di Yale, mostrò come l'algebra booleana e le operazioni binarie potessero rappresentare il cambiamento circuitale e l'inizio dell'innovazione digitale nelle reti di telecomunicazione.

Un ulteriore passo importante fu l'articolo di Alan Turing redatto nel 1936, On Computable Numbers, With An Application To The Entscheidungsproblem[4], che pone le basi per concetti quali calcolabilità, computabilità, macchina di Turing, definizioni cardine per i calcolatori sino ai giorni nostri. In seguito, nel 1943 McCulloch e Pitts crearono ciò che viene ritenuto il primo lavoro inerente all'intelligenza artificiale[5]. Tale sistema impiega un modello di neuroni artificiali nel quale lo stato di tali neuroni può essere “acceso” o “spento,” con un passaggio ad “acceso” in presenza di stimoli causati da un numero sufficiente di neuroni circostanti.

McCulloch e Pitts arrivarono quindi a mostrare, ad esempio, che qualsiasi funzione computabile può essere rappresentata da qualche rete di neuroni, e che tutti i connettivi logici (“e”, “o”, ...) possono essere implementati da una semplice struttura neurale. Sette anni più tardi, nel 1950, due studenti dell'università di Harvard, Marvin Minsky e Dean Edmonds, crearono quella che viene riconosciuta come la prima rete neurale artificiale, conosciuta con il nome di SNARC.

La nascita effettiva della disciplina (1956)
Nel 1956, nel New Hampshire, al Dartmouth College, si tenne un convegno al quale presero parte alcune delle figure di spicco del nascente campo della computazione dedicata allo sviluppo di sistemi intelligenti: John McCarthy, Marvin Minsky, Claude Shannon e Nathaniel Rochester. Su iniziativa di McCarthy, un team di dieci persone avrebbe dovuto creare in due mesi una macchina in grado di simulare ogni aspetto dell'apprendimento e dell'intelligenza umana. Ad aderire a tale iniziativa furono alcuni ricercatori, tra cui anche Trenchard More di Princeton, Arthur Samuel di IBM, e Ray Solomonoff e Oliver Selfridge del MIT. Nello stesso convegno, un'altra iniziativa catalizzò l'attenzione oltre al progetto di McCarthy: il programma di Allen Newell e Herbert Simon. Questi due ricercatori, a differenza di McCarthy, avevano già un programma capace di qualche forma di ragionamento, conosciuto con il nome di Logic Theorist, o LP, in grado di dimostrare teoremi partendo dai principi della matematica. Sempre nello stesso convegno, McCarthy introdusse l'espressione intelligenza artificiale, che segnò, in maniera indelebile, la nascita effettiva di tale disciplina, conferendole una natura propria.

Prime grandi aspettative (1950-1965)
Il programma creato da Newell e Simon permise loro di progredire e creare un programma chiamato General Problem Solver, o GPS. A differenza del LP, il GPS fu ideato con lo scopo di imitare i processi di risoluzione dei problemi utilizzati dagli esseri umani (nello specifico la cosiddetta "euristica mezzi-fini"[6]). Nei ristretti casi nel quale il programma poteva operare, si notò che l'approccio con il quale il programma considerava gli obiettivi e le azioni era assimilabile a un umano. Negli stessi anni, presso l'IBM, Rochester con dei suoi colleghi cominciò a sviluppare altri programmi capaci di ragionamento.

Nel 1959, Herbert Gelemter creò il Geometry Theorem Prover, un programma in grado di dimostrare teoremi di geometria complessi. L'anno precedente, presso il MIT, McCarthy diede un altro contributo al campo dell'intelligenza artificiale definendo quello che per trent'anni fu riconosciuto come il linguaggio di programmazione dominante per la realizzazione dei sistemi di intelligenza artificiale: il Lisp. Oltre a ciò, McCarthy scrisse un documento intitolato Programs with Common Sense[7], nel quale descrive un programma ideale, chiamato Advice Taker, che può essere visto come il primo sistema intelligente completo.

Minsky, durante il suo periodo al MIT, coordinò la creazione di programmi per affrontare quelli che vengono chiamati micro mondi, ovvero problemi limitati e descritti da asserzioni che richiedevano l'utilizzo di ragionamento per essere risolti. Tra questi, il programma di James Slagle del 1963, SAINT, era in grado di risolvere problemi riguardo al calcolo integrale in forma chiusa, tipici del primo anno del college.

Prime difficoltà (1966-1969)
Tra le varie aspirazioni da parte dei ricercatori vi era principalmente quella di creare macchine in grado di esibire capacità di ragionamento simili a quelle umane. Ad esempio, Herbert Simon, nel 1957, stimò che nel giro di dieci anni ci sarebbero state macchine in grado di competere con i campioni di scacchi (previsione che si avvererà, ma dopo quarant'anni). Queste aspirazioni, però, dovettero scontrarsi con alcune difficoltà: prime fra tutte, l’assoluta mancanza di conoscenza semantica relativa ai domini trattati dalle macchine, in quanto la loro capacità di ragionamento si limitava a una vera manipolazione sintattica. A causa di questa difficoltà, nel 1966 il governo degli Stati Uniti d'America interruppe i fondi per lo sviluppo delle macchine traduttrici. Un ulteriore problema fu l'impossibilità del trattare molti problemi che l'intelligenza artificiale si era proposta. Questo perché si riteneva che “scalare” le dimensioni di un problema fosse solo una questione di hardware e memoria.

Questo tipo di ottimismo fu presto spento quando i ricercatori fallirono nel dimostrare teoremi a partire da più di una dozzina di assiomi. Si capì quindi che il fatto di disporre di un algoritmo che, a livello teorico, fosse in grado di trovare una soluzione a un problema non significava che un corrispondente programma fosse in grado di calcolarla effettivamente a livello pratico. Un terzo tipo di difficoltà furono le limitazioni alla base della logica, nel senso di ragionamento, dei calcolatori. Nel documento di Minsky e Papert, intitolato Perceptrons (1969), si mostrò che, nonostante un percettrone (una semplice forma di rete neurale) fosse in grado di apprendere qualsiasi funzione potesse rappresentare, un percettrone con due input non era in grado di rappresentare una funzione che riconoscesse quando i due input sono diversi.

Sistemi basati sulla conoscenza (1969-1979)
Queste difficoltà portarono a definire gli approcci adottati dalle macchine come approcci deboli, che necessitavano quindi di una conoscenza maggiore inerente al campo di applicazione. Nel 1969, grazie a Ed Feigenbaum (studente di Herbert Simon), Bruce Buchanam e Joshua Lederberg, venne creato il programma DENDRAL. Tale programma era in grado, a partire dalle informazioni sulla massa molecolare ricavate da uno spettrometro, di ricostruire la struttura di una molecola. Questo programma fu quindi il primo dei sistemi basati su un uso intensivo della conoscenza, che arrivarono più tardi a inglobare tutti i concetti teorizzati da McCarthy per l'Advice Taker. Successivamente, Feigenbaum cominciò insieme con altri ricercatori di Stanford l'Heuristic Program Project (HPP), al fine di estendere gli scenari applicativi di questi sistemi, cominciando con il sistema MYCIN nell'ambito delle diagnosi delle infezioni sanguigne. Si cominciò quindi a teorizzare dei sistemi conosciuti come sistemi esperti, ovvero in grado di possedere una conoscenza esperta in un determinato scenario di applicazione. Si trattava di sistemi in cui l’uomo trasferiva direttamente la propria conoscenza alla macchina, stabilendo mediante regole logiche quali fossero le scelte da prendere in determinati contesti.[8]

Dall'ambiente accademico all'industria (1980-1985)
Il primo sistema di intelligenza artificiale utilizzato in ambito commerciale fu R1, utilizzato dalla Digital Equipment nel 1982. Lo scopo del programma era quello di aiutare a configurare gli ordini per nuovi computer. Nel 1986, fu in grado di far risparmiare alla compagnia 40 milioni di dollari all'anno. Anche la DuPont utilizzò sistemi simili, risparmiando circa dieci milioni di dollari all'anno. Negli anni '80 dello scorso secolo, quasi ogni grande azienda americana aveva un proprio sistema esperto in operazione e stava studiando sistemi più avanzati. Nel 1981 in Giappone venne annunciato il progetto Fifth Generation, un piano di dieci anni con l'intento di costruire sistemi intelligenti basati su Prolog. In risposta, gli Stati Uniti d'America crearono la Microelectronics and Computer Technology Corporation (MCC), come consorzio di ricerca al fine di garantire la competitività a livello nazionale. In Inghilterra, il rapporto Alvey recuperò i fondi tagliati dal rapporto Lighthill, che nel 1973 portò il governo britannico alla decisione di interrompere il supporto verso la ricerca nell'ambito dell'intelligenza artificiale. Questi progetti però non raggiunsero gli scopi previsti. L'industria dell'intelligenza artificiale raggiunse nel 1988 una cifra dell'ordine di miliardi di dollari, includendo centinaia di aziende che stavano creando sistemi esperti, robot e software e hardware specializzati in questi settori.

Il ritorno delle reti neurali (1986-)

A metà degli anni ottanta dello scorso secolo fu reinventato l'algoritmo di apprendimento per reti neurali chiamato back-propagation, inizialmente ideato nel 1969 da Bryson e Ho. L'algoritmo fu applicato a molti problemi relativi all'apprendimento, inerenti sia al lato dell'informatica sia a quello della psicologia. I cosiddetti modelli "connessionisti" per la realizzazione di sistemi intelligenti furono visti come alternative ai modelli simbolici ideati da Newell e Simon, da McCarthy e dai loro collaboratori. Tali modelli cercarono di dare risposta a quelle domande alle quali i precedenti modelli non erano riusciti, ma in parte fallirono anch'essi. Di conseguenza, i modelli basati sull'approccio simbolico e quelli con un approccio connessionista furono visti come complementari.

L'intelligenza artificiale moderna (1986-)

Oggigiorno i sistemi intelligenti sono presenti in ogni campo, anche nelle attività quotidiane e primeggiano nei giochi, come teorizzato anni prima dagli esponenti dell'intelligenza artificiale. Vi sono programmi che sono stati in grado di confrontarsi con campioni di scacchi, quali Deep Blue; altri che sono stati impiegati nelle missioni spaziali, come nel 1998 quando la NASA utilizzò un programma chiamato Remote Agent in grado di gestire le attività relative a un sistema spaziale; alcune auto sono oggi dotate di un sistema in grado di guidarle senza l'uso di un conducente umano, quindi in maniera del tutto autonoma. Nell'ambito di scenari più quotidiani si pensi, invece, ai termostati per il riscaldamento e l'aria condizionata in grado di anticipare il cambio di temperatura, gestire i bisogni degli abitanti e di interagire con altri dispositivi. In campo economico, particolarmente sensibile al cambiamento è il tasso di occupazione in generale,[9] come nella tecnofinanza dove avviene la più profonda rivoluzione.

Utilizzo di Python

Il linguaggio di programmazione più diffuso nell'ambito della ricerca e dello sviluppo di sistemi di intelligenza artificiale è Python. Grazie alla sua semplice sintassi, questo linguaggio consente una rapida prototipazione delle reti neurali, facendo così risparmiare tempo ai ricercatori. Questo è un aspetto molto importante perché tipicamente le reti neurali richiedono molto tempo per essere allenate (da qualche ora, fino a qualche giorno o settimana).

Originariamente Python non è stato progettato per essere efficiente, ma grazie a Cython è stato possibile implementare librerie in grado di eseguire codice direttamente in C. Siccome le operazioni svolte tra i dati in input ai modelli di intelligenza artificiale, e i relativi parametri, sono spesso delle operazioni tra tensori (ad esempio prodotti tra tensori), è emerso che l'impiego di GPU per l'allenamento di questi modelli è molto vantaggioso, e di conseguenza sono nati frameworks come PyTorch o Tensorflow. Questi frameworks, importati in Python come librerie, consentono la realizzazione, l'allenamento e l'inferenza di modelli di intelligenza artificiale, fornendo la possibilità di eseguire tali modelli su CPU, GPU, o eventualmente anche TPU.

Principi di Asilomar
Nel 2017 a seguito del convegno di esperti mondiali di intelligenza artificiale Conferenza di Asilomar sulla IA Benefica, promosso dal Future of Life Institute, è stato redatto con amplissimo consenso un vademecum con 23 principi per affrontare le problematiche etiche, sociali, culturali e militari dell'IA. Il documento è stato sottoscritto subito da oltre 800 esperti e in seguito da altre migliaia[


Codice etico UE per l'intelligenza artificiale

Partendo dalla premessa per cui i governi devono garantire l'impiego dell'intelligenza artificiale nel massimorispetto dell'etica, nell'aprile del 2019, l'Unione Europea ha elaborato il suo codice etico[15], che contiene le linee guida su utilizzo e sviluppo di sistemi di intelligenza artificiale. Il documento, che è stato predisposto da un gruppo di 52 esperti, rappresentati da informatici, ingegneri ma anche giuristi, filosofi, industriali, matematici, ha avuto un iter lungo e varie fasi di approfondimento[16].

Il punto di partenza dell'intero documento, e di tutti i principi giuridici che ne sono scaturiti, è che l'intelligenza artificiale deve avere l'uomo al centro e deve essere al servizio del bene comune per migliorare il benessere e garantire la libertà. Per prima cosa, il gruppo di esperti ha identificato le fondamenta giuridiche sulle quali il codice dovesse poggiare ricercandole nei Trattati UE, nella Carta dei Diritti e nella legge internazionale dei Diritti Umani. Da questa analisi sono stati individuati quei diritti inderogabili che, nell'Unione Europea, devono essere rispettati per l'intelligenza artificiale, vale a dire:
- Rispetto per la dignità dell'uomo
- Libertà dell'individuo
- Rispetto per la democrazia e per la giustizia
- Eguaglianza e non discriminazione
- Diritti dei cittadini

A questo punto è stato possibile dare indicazioni su quali fossero i principi etici da seguire nell'Unione per garantire che i sistemi di intelligenza artificiale siano sfruttati in modo affidabile, ovvero rispetto per l'autonomia dell'uomo, prevenzione del danno, equità e correttezza[17].

L'ultima fase di lavoro del gruppo di esperti è stata quella di redigere le linee guida UE del codice etico cui aziende, ricercatori e le comunità in generale dovranno attenersi e che rappresentano la traduzione operativa e la sintesi dei diritti fondamentali e dei principi sopra elencati[18].

Linee guida
- Supervisione umana: l'intelligenza artificiale deve essere al servizio dell'uomo e non deve invece ridurne, limitarne o fuorviarne l'autonomia; inoltre, non devono essere sviluppati sistemi che mettano a rischio i diritti fondamentali dell'uomo. La persona deve restare autonoma e in grado di supervisionare il sistema stesso.
- Solidità tecnica e sicurezza: gli algoritmi devono essere affidabili e sviluppati in modo tale che la sicurezza non venga messa in pericolo durante l'intero ciclo di vita del sistema.
- Privacy e governance dei dati: i cittadini devono sempre essere informati dell'utilizzo dei propri dati personali nel massimo rispetto della normativa UE sulla privacy per l'intero ciclo di vita del sistema che fa uso dell'intelligenza artificiale.
- Trasparenza: significa tracciabilità dei sistemi di intelligenza artificiale. Tutti i dati utilizzati, inclusi gli algoritmi, vanno documentati; solo così si potranno capire i motivi per cui, ad esempio, una decisione basata sull'intelligenza artificiale è stata presa in modo errato.- Diversità, assenza di discriminazione, correttezza: i sistemi di intelligenza artificiale devono prendere in considerazione tutte le capacità e le abilità umane, garantendo l'accessibilità a tutti.
- Benessere sociale e ambientale: i sistemi di intelligenza artificiale devono essere utilizzati per sostenere cambiamenti ambientali positivi e perseguire obiettivi di sviluppo sostenibile.
- Responsabilità: devono essere adottati meccanismi di responsabilità nel riportare i dati e gli algoritmi utilizzati nei sistemi di intelligenza artificiale. Questo processo di valutazione consente di minimizzare eventuali impatti negativi.

Artificial Intelligence Act

L'Unione Europea nell'aprile 2021 ha elaborato una proposta di legge che prende il nome di AI Act (https://
artificialintelligenceact.eu/). La legge[20] classifica l'utilizzo delle intelligenze artificiali in base a 3 livelli di
rischio. Il primo include le applicazioni e i sistemi che generano rischi inaccettabili, ad esempio attività di
social scoring da parte del governo come quelle svolte in Cina. Il secondo include applicazioni ad alto
rischio, come degli strumenti di analisi CV che classificano i candidati per il lavoro, sono soggetti a specifici
requisiti legali. Infine, applicazioni non esplicitamente vietate o inserite nella lista ad alto rischio non
subiscono una regolamentazione.

Algoretica

Le tre principali religioni abramitiche, Microsoft e IBM, all'inizio del 2023 si sono incontrati in Vaticano alla
Rome Call, per la richiesta congiunta di un'algoretica[21] (la riflessione etica sull’uso degli algoritmi) che
guidi la progettazione dell'Intelligenza Artificiale. L'etica sull'uso degli algoritmi si basa su sei principi:
- Trasparenza
- Inclusione
- Responsabilità
- Imparzialità
- Affidabilità
- Sicurezza e privacy

Ricerca

Il problema complesso dello sviluppare sistemi che esibiscono comportamenti intelligenti è stato affrontato operando una scomposizione in sotto-problemi, ognuno con uno specifico ambito di ricerca. Ogni sotto- problema consiste nello studiare particolari abilità e proprietà che caratterizzano il sistema intelligente.
Relativamente all'ambito di applicazione di un determinato sistema intelligente questo presenterà soluzioni più o meno evolute per ogni sotto-problema.


Intelligenza artificiale forte e debole
Una primaria distinzione in seno alla ricerca nel campo dell'intelligenza artificiale è quella di intelligenza artificiale debole e intelligenza artificiale forte a secondo che vengano riprodotte solo alcune o tutte le funzionalità della mente umana.


Deduzione, ragionamento e problem solving

Inizialmente i ricercatori si concentrarono sullo sviluppo di algoritmi che imitassero fedelmente i ragionamenti impiegati dagli esseri umani per risolvere giochi o realizzare deduzioni logiche in modo da oterli integrare all'interno dei sistemi intelligenti. Tali algoritmi solitamente si basano su una rappresentazione simbolica dello stato del mondo e cercano sequenze di azioni che raggiungano uno stato desiderato. Evoluzioni di questi algoritmi vennero realizzati tenendo in considerazione aspetti più complessi come l'incertezza o l'incompletezza delle informazioni, includendo concetti provenienti dalla probabilità, dalla statistica e dall'economia.

Per difficoltà legate alla complessità intrinseca dei problemi in esame, gli algoritmi per la loro risoluzione possono a volte richiedere enormi risorse computazionali. L'ottimizzazione degli algoritmi ricopre una priorità assoluta all'interno della ricerca in questo ambito.

Rappresentazione della conoscenza

La rappresentazione della conoscenza e l'ingegneria della conoscenza costituiscono contributi centrali per la ricerca nell'ambito dell'intelligenza artificiale.

In particolare, queste discipline si focalizzano su quale tipo di conoscenza è necessario o opportuno integrare all'interno di un sistema intelligente, e sul come rappresentare i diversi tipi di informazione. Fra le cose che un sistema intelligente ha la necessità di rappresentare troviamo frequentemente oggetti, proprietà, categorie e relazioni fra oggetti, situazioni, eventi, stati, tempo, cause ed effetti, conoscenza posseduta da altri. La rappresentazione e l'ingegneria della conoscenza vengono spesso associate alla disciplina filosofica dell'ontologia.

La conoscenza e la sua rappresentazione sono cruciali soprattutto per quella categoria di sistemi intelligentiche basano il loro comportamento su una estensiva rappresentazione esplicita della conoscenza dell'ambiente in cui operano.


Pianificazione

Per permettere ai sistemi intelligenti di prevedere e rappresentare stati del mondo futuri e per prendere decisioni al fine di raggiungere tali stati massimizzando il valore atteso delle azioni, essi devono essere in grado di definire degli obiettivi e di perseguirli.
Nei problemi classici di pianificazione, un sistema intelligente può assumere di essere l'unica entità a operare nell'ambiente e può essere assolutamente sicuro delle conseguenze di ogni azione compiuta. Se non è l'unico attore nell'ambiente o se l'ambiente non è deterministico un sistema intelligente deve costantemente monitorare il risultato delle proprie azioni e aggiornare le predizioni future e i propri piani.


Apprendimento

L'apprendimento automatico è la disciplina che studia algoritmi capaci di migliorare automaticamente le proprie prestazioni attraverso l'esperienza. È stato un ambito di ricerca cruciale all'interno dell'intelligenza artificiale sin dalla sua nascita.

L'apprendimento automatico è particolarmente importante per lo sviluppo di sistemi intelligenti
principalmente per tre motivi:
- Gli sviluppatori di un sistema intelligente difficilmente possono prevedere tutte le possibili situazioni in cui il sistema stesso si può trovare a operare, eccetto per contesti estremamente semplici.
- Gli sviluppatori di un sistema intelligente difficilmente possono prevedere tutti i possibili cambiamenti dell'ambiente nel tempo.
- Un'ampia categoria di problemi può essere risolta più efficacemente ricorrendo a soluzioni che coinvolgono l'apprendimento automatico. Questa categoria di problemi include, ad esempio, il gioco degli scacchi e il riconoscimento degli oggetti.


Elaborazione del linguaggio naturale

La capacità di elaborare il linguaggio naturale fornisce ai modelli di intelligenza artificiale di stimare con ottime probabilità la parola o la frase che segue il testo fornito in input, e di estrarne il contesto. Questa tecnica consente di ottenere risultati migliori rispetto a tecniche tradizionali quando si tratta di svolgere
ricerca di informazioni, ottenere risposta a domande,tradurre o analizzare testi. L'architettura tipicamente impiegata per questo compito è il Transformer[22], che grazie al meccanismo dell'attenzione è in grado di catturare appieno il contesto del testo fornito in input.

La difficoltà principale di questo processo è l'intrinseca ambiguità che caratterizza i linguaggi naturali, per questo motivo le soluzioni richiedono un'estesa conoscenza del mondo e una notevole abilità nel manipolarlo.


Movimento e manipolazione

La robotica è una disciplina strettamente correlata con l'intelligenza artificiale.

I robot possono essere considerati sistemi intelligenti per tutti quei compiti che richiedono capacità di livello cognitivo per la manipolazione o lo spostamento di oggetti e per la locomozione, con i sotto-problemi della localizzazione (determinare la propria posizione e quella di altre entità nello spazio), della costruzione di
mappe (apprendere le caratteristiche dello spazio circostante), e della pianificazione ed esecuzione dei movimenti.

Agente intelligente

Il concetto di agente intelligente (o agente razionale) è centrale in molti degli approcci più comuni all'intelligenza artificiale.

Un agente è un'entità in grado di percepire l'ambiente attraverso l'utilizzo di sensori e in grado di agire sull'ambiente attraverso l'utilizzo di attuatori. Ogni agente è quindi associato a una sequenza di percezioni, intesa come la cronologia completa di tutti i rilevamenti effettuati da ciascun sensore, e a una funzione agente, che specifica il comportamento dell'agente associando a ogni sequenza di percezioni un'azione da
compiere.

Definita misura della performance una funzione che associa a ogni stato (o sequenza di stati) dell'ambiente un valore di utilità, un agente è intelligente (o razionale) se per ogni possibile sequenza di percezioni la sua funzione agente lo porta a compiere sempre l'azione che massimizza il valore atteso della misura della performance, data la sua conoscenza definita dalla sequenza di percezioni stessa e dalla conoscenza integrata nell'agente.
Esistono metodologie differenti per l'implementazione concreta della funzione agente, ciascuna più o meno adatta al tipo di ambiente in cui è posto l'agente.

Applicazioni

L'intelligenza artificiale è stata impiegata in un'ampia varietà di campi e applicazioni come la medicina, il mercato azionario, la robotica, la legge, la ricerca scientifica, l'analisi dei dati, i giocattoli e perfino lo sviluppo di nuovi robot usando la potenza di calcolo di un personal computer.[24][25] In alcune applicazioni, l'intelligenza artificiale si è radicata a tal punto all'interno della società o dell'industria da non essere più percepita come intelligenza artificiale.[26] Essa trova applicazione anche nelle smart city: gestione dei flussi (veicolari o turistici), operatività delle reti (telecomunicazioni ed energia), acquisti online e telelavoro.[27]
Inoltre, trova applicazione nell'e-procurement, ad esempio nella ricerca e selezione di nuovi fornitori.[28]

Anche nel campo dell'informatica stessa, molte soluzioni sviluppate originariamente per rispondere a problemi o necessità dell'intelligenza artificiale sono state adottate da altre discipline e non vengono più considerate parte dell'intelligenza artificiale. In particolare il time-sharing, l'interprete (informatica),l'interfaccia grafica, il mouse, la struttura dati lista concatenata, la programmazione funzionale, la programmazione simbolica, la programmazione dinamica e la programmazione orientata agli oggetti.[29]

Il primo utilizzo dell'intelligenza artificiale nelle banche è datato 1987 quando la Security Pacific National Bank negli USA organizzò una task force per la prevenzione delle frodi legate all'utilizzo non autorizzato delle carte di credito. Attualmente, e non solo in ambito bancario, le reti neurali vengono utilizzate per identificare fenomeni non riconducibili a un comportamento nominale e che richiedono un intervento umano.

Le reti neurali sono anche largamente impiegate per supportare le diagnosi mediche, e molte altre
applicazioni sono attualmente in sviluppo.

L'intelligenza artificiale è largamente utilizzata per la realizzazione di assistenti automatici online principalmente dalle compagnie telefoniche e di telecomunicazione, con l'intento di ridurre i costi di assunzione e formazione del personale.

Anche nell'ambito dei trasporti l'utilizzo dell'intelligenza artificiale sta aumentando rapidamente:
Applicazioni della logica fuzzy sono state impiegate nella realizzazione di cambi di velocità per le automobili. Le automobili a guida autonoma sviluppate da Google e Tesla fanno largamente uso di tecniche di intelligenza artificiale.

L’intelligenza artificiale viene anche impiegata nel campo della videosorveglianza. Gli algoritmi consentono il riconoscimento degli oggetti presenti nella scena al fine di generare allarmi.

Ultimo, ma non per importanza, è l'applicazione di reti neurali complesse nella generazione di testi, o meglio, nella trasformazione di un input generalmente testuale in un output anch'esso espresso in caratteri. In particolar modo negli ultimi anni, OpenAI ha rilasciato numerose versioni del suo "modello" denominato GPT, il quale ha riscontrato notevole successo e scalpore. Attraverso questo modello basato su una particolare rete neurale, è stato possibile generare dei racconti, riassumere automaticamente dei testi, tradurre in maniera sempre più precisa da una lingua all'altra. Attraverso questa disciplina le applicazioni sono le più
disparate, tra cui, degno di nota e a forte impatto sociale, quello riguardo al binomio giornalismo e scrittura. Il Washington Post ad esempio, gìà nel 2017 dichiarò di aver pubblicato in un anno 850 news elaborate da un'intelligenza artificiale. Il giornale canadese The Globe and Mail invece è interamente diretto da una intelligenza artificiale[53]. Un altro utilizzo di questo modello trova riscontro nei tool di assistenza alla scrittura e generazione automatica di testi.


Il futuro dell'intelligenza artificiale in Italia
Benché le aziende italiane nel complesso non abbiano ancora una visione omogenea sul tema, si
individuano già aree di sviluppo particolarmente interessanti:

    Smart home speaker
Si tratta di assistenti vocali intelligenti in grado di gestire oggetti intelligenti presenti in casa. Sono stati
introdotti di recente, ma il loro mercato in Italia vale già 60 milioni di euro e il valore sembra destinato a
crescere: in un futuro non troppo lontano, questi assistenti potrebbero fungere da canale con cui veicolare
servizi e applicazioni legate al mondo dell'AI, creando nuove opportunità di sviluppo per le aziende del
settore.

    Robot intelligenti
A questa categoria appartengono i collaborative robot e gli AGV (Automated Guided Vehicle). I primi
collaborano con un operatore umano e sono in grado di adattare il proprio comportamento agli stimoli
esterni, mentre i secondi si adattano all'ambiente esterno muovendosi in autonomia, senza il supporto di
guide fisiche o percorsi predeterminati.[59]
    Tutor Intelligenti
A questa categoria appartengono gli avatar degli Edugames oppure dei robot che all'interno dei musei, e
altri luoghi dell'apprendimento, guidano i discenti-visitatori e fungere dai docenti-educatori
artificiali[60][61][62][63].



AI for Good
AI for Good è la piattaforma informatica dell'ONU che ha l'obiettivo di promuovere il dialogo nella
comunità scientifica finalizzato allo sviluppo di progetti concreti nell'ambito dell'intelligenza artificiale,
mediante un uso etico e orientato al bene comune di questa famiglia di tecnologie.

A partire dal 2017, AI for Good organizza ogni anno un evento globale, la cui quarta edizione è stata fatta il
21 settembre 2020 a Ginevra, in Svizzera. L'iniziativa operando in relazione a obiettivi di respiro
globale[64][65], in particolare riguardo allo sviluppo sostenibile, e si propone di ottenere risultati più immediati
e concreti rispetto ai documenti programmatici e di indirizzo generalmente prodotti dai meeting dell'ONU.

Le applicazioni di intelligenza artificiale sono state classificate in tre macrocategorie: AI per la Terra (AI for
Earth)[66][67], AI per fini umanitari (Humanitarian AI)[68][69] e AI per l'assistenza sanitaria (AI for
Healthcare).[70]

Il primo AI for Good Global Summit si è tenuto dal 7 al 9 giugno 2017 a Ginevra[71][72][73] è stata la
creazione di un focus group dell'ITU-T in tema di apprendimento automatico per la tecnologia di
connessione 5G.[74]

Il secondo AI for Good Global Summit si è svolto dal 15 al 17 maggio 2018 presso la sede dell'ITU a
Ginevra, e ha prodotto un totale di 35 progetti[75], anche in collaborazione con l'OMS per la categoria AI 4
Health (FG-AI4H).[76][77][78]
Fra i relatori erano presenti Roger Penrose e Samantha Cristoforetti.[79][80] In tale occasione, è stato attivato
un repository dei progetti di AI for Goods e dei relativi esempi finalizzato agli obbiettivi dello sviluppo
sostenibile[81][82], mentre l'ITU ha lanciato la rivista ICT Discoveries[83], la cui prima edizione straordinaria è
stata dedicata all'intelligenza artificiale.[84]

Il terzo AI for Good Global Summit ha avuto luogo dal 28 maggio al 31 maggio 2019, sempre nella città
svizzera che è sede dell'ONU[85], relativamente alle applicazioni civili e militari dell'AI nello spazio, quali
ad esempio le previsioni meteorologiche affidabili entro un orizzonte temporale di 2 settimane, la previsione
di asteroidi e corpi celesti in rotta di collisione con la Terra, il monitoraggio delle migrazioni animali di
balene o specie in via di estinzione, la gestione satellitare di servizi basati sulla geolocalizzazione (come il
controllo automatico di autoveicoli privi di guidatore).[86]



Critiche e controversie
Una maggiore attenzione è rivolta alle implicazioni etiche, ambientali e sociali dell'intelligenza artificiale e
alla necessità di aumentare la trasparenza e la responsabilità delle grandi aziende tecnologiche per i loro
algoritmi. Le principali critiche si riferiscono a:

    Pregiudizio algoritmico[87]
    La mancanza di responsabilità per i risultati generati dagli algoritmi "black-box”[88]
    Approvvigionamento non etico di minerali rari utilizzati nei dispositivi alimentati dall'IA[89]
    Impronta ambientale dei datacenter, il loro utilizzo di energia e acqua[90][91]
    Sfruttamento del lavoro digitale "clickwork" coinvolto nell'etichettatura dei dati per IA training
    e nella moderazione dei contenuti[90]
    Manipolazione algoritmica delle preferenze di consumo e di voto degli utenti[92]


Trasparenza algoritmica e segreto industriale
Negli ultimi anni, a causa della crescente presenza di AI nella società, ci sono stati dei tentativi di normare e
integrare l'utilizzo delle intelligenze artificiali all'interno del quadro normativo europeo, con particolare
attenzione al principio di trasparenza algoritmica, che può essere definito come "l'obbligo, gravante sui
soggetti che adottano decisioni con l'ausilio di sistemi automatizzati di trattamento dei dati, di fornire ai
destinatari una spiegazione comprensibile delle procedure utilizzate e di motivare sotto questo profilo le
decisioni assunte"[93]. Il mancato rispetto della trasparenza violerebbe espressamente l'art. 111 Cost. e il
diritto alla difesa ex art. 24 Cost. Inoltre, è stata ribadita nel 2017, dalla Dichiarazione di Asilomar,
l'esigenza di garantire la massima trasparenza in ambito di decisioni giudiziarie, in caso di coinvolgimento di
sistemi autonomi. Più di recente, al fine di istituire regole armonizzate a livello europeo per lo sviluppo e
l'utilizzo dei sistemi di AI, il 21 aprile 2021 la Commissione Europea ha pubblicato una Proposta di
Regolamento[94], basandosi su un quadro giuridico già esistente, senza minare la coerenza con le altre
norme europee vigenti. Nella suddetta proposta è presente una classificazione delle intelligenze artificiali,
con particolare focus sulle "pratiche di intelligenze vietate" e "sistemi AI ad alto rischio". Il capo 1 del titolo
III fissa le regole di classificazione e individua due categorie principali di sistemi di IA ad alto rischio:

    i sistemi di IA destinati ad essere utilizzati come componenti di sicurezza di prodotti soggetti
    a valutazione della conformità ex ante da parte di terzi;
    altri sistemi di IA indipendenti che presentano implicazioni principalmente in relazione ai
    diritti fondamentali esplicitamente elencati nell'allegato III.
La Proposta segue un approccio basato sul rischio e impone oneri normativi soltanto laddove un sistema di
IA possa comportare rischi alti per i diritti fondamentali e la sicurezza. Per altri sistemi di IA non ad alto
rischio sono imposti soltanto obblighi di trasparenza limitati, ad esempio in termini di fornitura di
informazioni per segnalare l'utilizzo di un sistema di IA nelle interazioni con esseri umani. Per i sistemi di
IA ad alto rischio, i requisiti di qualità elevata dei dati, documentazione e tracciabilità, trasparenza,
sorveglianza umana, precisione e robustezza sono strettamente necessari per attenuare i rischi per i diritti
fondamentali e la sicurezza posti dall'IA e che non sono oggetto di altri quadri giuridici in vigore. Il titolo IV
si concentra su determinati sistemi di IA, al fine di considerare interazioni analitiche con esseri umani, in
grado di interpretare emozioni, comportamenti e abitudini sociali o casi di manipolazione di contenuti
("deep fake") che richiedono la piena consapevolezza dell'essere umano. Ciò consente alle persone di
compiere scelte informate o di compiere un passo indietro rispetto a una determinata situazione. Tuttavia,
nel Considerando n.63 del GDPR si precisa che tale diritto di conoscibilità dell'interessato non dovrebbe
ledere i diritti e le libertà altrui, compreso il segreto industriale e aziendale e la proprietà intellettuale. Questo
porta ad un contrasto sulla quantità di informazioni da rivelare sugli algoritmi coinvolti nel processo
decisionale e sul tipo di processi da rendere trasparenti. La possibilità di fare reverse engineering diventa
infatti un rischio concreto di perdita economica per un'azienda nel settore AI, oltre al concreto rischio di
abbassamento del livello di sicurezza causato dal "data poisoning". I sistemi AI, infatti, si basano su una
significativa raccolta e analisi di basi di dati come fase di preparazione e monitoraggio degli output rispetto
al modello di dati atteso. Attraverso tale meccanismo, le istruzioni fornite non sono completamente
predeterminate, ma viene consentito alla macchina di apprendere dall'esperienza, tramite l'inserimento dei
dati; questi sistemi, quindi, non si limitano a seguire le istruzioni del programmatore, ma possono andare
verso soluzioni impreviste, in relazione ai dati che acquisiscono durante il loro funzionamento. Ancora più
oscuri possono risultare i sistemi di deep learning (una categoria del machine learning), basati sulle reti
neurali artificiali, il cui funzionamento risulta ad oggi per lo più inspiegabile in maniera teorica, ma la cui
efficacia risulta provata a livello empirico. Questa "black box" che si genera tra input ed output complica
ulteriormente la garanzia sulla trasparenza dei dati. Sull'argomento si è espresso anche il GDPR
(Regolamento UE 2016/679) tramite gli articoli 13 e 15 in cui si manifesta l'esigenza dell'utente interessato
a conoscere l'esistenza di tutti i processi decisionali automatizzati che lo riguardino, nonché la conseguenza
prevista di tale trattamento dei dati. Spesso viene fatto riferimento alla significatività dell'algoritmo, ovvero
quanto ha contato l'algoritmo stesso in una specifica decisione all'interno, ad esempio, di un processo
amministrativo. Un caso emblematico è rappresentato dal caso COMPAS[95]. Nelle controversie sulla
trasparenza algoritmica, una delle soluzioni attuali possibili è l'AI spiegabile. L'AI Spiegabile nasce per
garantire la tracciabilità dei calcoli, è la risposta alla "black box" a cui si fa riferimento per indicare
l'impossibilità di seguire in maniera precisa i passaggi nell'ottenere un risultato. Tuttavia l'Explainable AI
presenta vantaggi e svantaggi: rallenta il processo di elaborazione dei dati, richiede formazione del
personale e ha un costo elevato poiché deve garantire un alto livello di precisione della previsione e, per
verificare ciò, vengono eseguite più simulazioni e confronti tra output con i risultati del dataset di
formazione.


Disoccupazione
Secondo il report intitolato The Potentially Large Effects of Artificial Intelligence on Economic Growth,
pubblicato da Goldman Sachs nel marzo 2023, l'intelligenza artificiale in particolare la sua capacità di
generare contenuti senza l'intervento umano potranno garantire una crescita del 7% del PIL globale nei
prossimi 10 anni. Tuttavia, essa è anche la causa prevedibile della perdita di 300 milioni di posti di lavoro
nei settori amministrativo, legale, finanziario e bancario.[96]

Secondo un rapporto del World Economic Forum del 2023, nei successivi 5 anni il 23% dei posti di lavoro
a livello mondiale subirà dei mutamenti a causa dell'intelligenza artificiale. L'automazione sostituirà l'81%
delle attività lavorative di intermediari di prestito, supervisori e impiegati d’ufficio.[97]

Tuttavia, storicamente non sempre l'automazione è sinonimo di disoccupazione. Un precedente storico è
rappresentato dall'introduzione del telaio meccanico che nel 1800 moltiplicò per 50 volte la produttività del
lavoro, riducendo negli Stati Uniti il fabbisogno di manodopera del 98%. Eppure il crollo dei costi causò
un'inaspettata esplosione della domanda, generando una quantità e varietà di posti di lavoro fino ad allora
impensabile.[98]



Regolamentazione

Unione europea

2018
L'Unione europea si è impegnata attivamente nello sviluppo e nell'applicazione dell'intelligenza artificiale
(IA) fin dal 2018.

Il 12 marzo 2018 il gruppo di lavoro sull'intelligenza artificiale dell'Agenzia per l'Italia Digitale ha emanato
il Libro bianco sull'intelligenza artificiale al servizio del cittadino[99]. Questo documento rappresenta un
importante strumento di soft-law non vincolante rivolto alle amministrazioni pubbliche, come scuole,
strutture sanitarie, Comuni, Tribunali e Ministeri, al fine di fornire raccomandazioni e indicazioni su come
sfruttare al meglio le opportunità offerte dall'Intelligenza Artificiale, minimizzando le criticità e gli aspetti
problematici, per sviluppare servizi pubblici sempre più centrati sul cittadino.

Il Libro Bianco identifica nove sfide chiave che le amministrazioni pubbliche devono affrontare per
garantire un utilizzo etico ed efficace dell'Intelligenza Artificiale. Queste sfide includono:
- l'etica
- la tecnologia
- le competenze
- il ruolo dei dati
- il contesto legale
- l'accompagnamento della trasformazione
- la prevenzione delle disuguaglianze
- la misurazione dell'impatto
- l'attenzione all'essere umano.

Attraverso una serie di raccomandazioni, il Libro Bianco fornisce linee guida concrete su come affrontare
queste sfide. Si concentra sull'importanza di promuovere una visione etica e responsabile nell'uso dell'IA,
sulla necessità di sviluppare competenze adeguate e favorire l'aggiornamento professionale, nonché
sull'importanza di creare un quadro normativo chiaro e coerente. Inoltre, il documento sottolinea
l'importanza di coinvolgere attivamente gli utenti e le parti interessate nella progettazione e nella valutazione
dei servizi basati sull'IA.

Riconoscendo il crescente impatto e il potenziale trasformativo dell'IA in vari settori, l'UE ha intrapreso
sforzi per promuovere un approccio strategico e normativo coerente. Nel quadro di questa iniziativa, il 10
aprile 2018, gli Stati Membri hanno emesso una Dichiarazione di Cooperazione sull'Intelligenza
Artificiale (https://digital-strategy.ec.europa.eu/en/library/coordinated-plan-artificial-intelligence).

La Dichiarazione, sebbene non vincolante, rappresenta un importante passo verso la creazione di un quadro
normativo uniforme nell'ambito dell'IA, basato su un approccio europeo comune. Gli Stati Membri hanno
riconosciuto la necessità di collaborare per affrontare le sfide e sfruttare le opportunità offerte dall'IA, al fine
di garantire un uso responsabile, etico e sicuro di questa tecnologia in Europa.

La Dichiarazione di Cooperazione sull'IA mira a promuovere la condivisione delle migliori pratiche, la
collaborazione nella ricerca e nello sviluppo, nonché l'adozione di standard comuni nel campo dell'IA.
Attraverso questa iniziativa, l'UE si impegna a sviluppare un approccio coerente e inclusivo per guidare
l'evoluzione dell'IA nel continente europeo.

Il 3 e 4 dicembre 2018 la Commissione europea per l'efficienza della giustizia approva a Strasburgo la
[https://rm.coe.int/carta-etica-europea-sull-utilizzo-dell-intelligenza-artificiale-nei-si/1680993348 Carta etica
europea sull’utilizzo dell’intelligenza artificiale nei sistemi giudiziari e negli ambiti connessi].
Il Piano coordinato sull'intelligenza artificiale[100], emanato dalla Commissione Europea il 7 dicembre
2018, è un atto non vincolante che fa seguito alla comunicazione L'intelligenza artificiale per l'Europa del
25 aprile 2018. Questo piano definisce le azioni da intraprendere nel settore dell'Intelligenza Artificiale per
gli anni 2019-2020.

La comunicazione L'intelligenza artificiale per l'Europa presenta la posizione della Commissione sulla
questione dell'intelligenza artificiale. Propone un approccio europeo basato su tre pilastri fondamentali:
anticipare gli sviluppi tecnologici e promuovere l'adozione sia nel settore pubblico che privato, prepararsi ai
cambiamenti socioeconomici derivanti dall'IA e garantire un quadro etico e giuridico appropriato.

Queste iniziative rivestono particolare importanza, poiché le comunicazioni della Commissione Europea
sono atti non vincolanti ma esprimono la posizione dell'organo chiave nella procedura legislativa dell'UE.
Ciò significa che esse influenzano l'agenda e le politiche dell'Unione europea nel campo dell'Intelligenza
Artificiale, promuovendo un approccio comune e coordinato tra i Paesi membri.


2019
L'8 aprile 2019, il Gruppo di esperti di Alto Livello sull'Intelligenza Artificiale, nominato dalla
Commissione Europea, ha emesso le "Ethics guidelines for trustworthy AI"[101] (Linee guida etiche per
un'IA affidabile). Questo documento rappresenta un importante strumento per lo sviluppo dell'approccio
dell'IA da parte della Commissione Europea e serve come risorsa per l'iniziativa legislativa in questo campo.

Le linee guida definiscono un approccio "human-centric" all'Intelligenza Artificiale, ponendo l'accento sulla
fiducia, l'etica e il rispetto dei valori umani. Esse identificano sette requisiti chiave per garantire l'affidabilità
dell'IA:

 1. Trasparenza: l'IA dovrebbe essere comprensibile e spiegabile.
 2. Imparzialità: l'IA dovrebbe evitare discriminazioni e pregiudizi.
 3. Inclusività: l'IA dovrebbe essere accessibile a tutti e rispettare la diversità.
 4. Responsabilità: le organizzazioni che sviluppano e utilizzano l'IA devono essere
    responsabili delle sue azioni e conseguenze.
 5. Precisione: l'IA dovrebbe fornire risultati accurati, affidabili e verificabili.
 6. Robustezza: l'IA dovrebbe essere in grado di resistere a errori o attacchi e garantire la
    sicurezza dei dati.
 7. Privacy: l'IA dovrebbe rispettare la privacy e proteggere le informazioni personali.[102]
Queste linee guida rappresentano un importante strumento per guidare lo sviluppo e l'applicazione dell'IA
nell'Unione Europea. Promuovendo l'affidabilità e l'eticità dell'IA, esse mirano a garantire che questa
tecnologia sia utilizzata nel rispetto dei diritti umani, della dignità e del benessere delle persone. Le linee
guida svolgono quindi un ruolo fondamentale nel plasmare il futuro quadro normativo e regolamentare
dell'IA nell'UE, guidando l'azione della Commissione e promuovendo un utilizzo responsabile e sicuro di
questa tecnologia emergente.


2021
Il 21 aprile 2021, la Commissione Europea ha emanato la "Proposta di regolamento contenente norme
armonizzate sull'intelligenza artificiale" (Artificial Intelligence Act)[103], che rappresenta il primo passo
nella procedura legislativa ordinaria dell'Unione Europea. Ai sensi dell'articolo 114 del Trattato sul
Funzionamento dell'Unione Europea, questa procedura prevede l'adozione congiunta, da parte del
Parlamento europeo e del Consiglio, di un regolamento, una direttiva o una decisione, generalmente su
proposta della Commissione.

La proposta di regolamento costituisce il primo quadro giuridico sull'intelligenza artificiale nell'UE, con
l'obiettivo di trasformare l'Unione in un hub globale per un'IA affidabile. La regolamentazione si basa su un
approccio denominato "risk-based", cioè basato sulla valutazione dei rischi. La proposta risponde alle
esplicite richieste del Parlamento europeo e del Consiglio europeo, che hanno sollecitato un'azione
legislativa per garantire un mercato interno efficiente per i sistemi di intelligenza artificiale, affrontando
adeguatamente sia i benefici che i rischi associati all'IA a livello dell'Unione.

L'obiettivo della proposta di regolamento è fornire una cornice normativa chiara e coerente per l'IA nell'UE,
garantendo al contempo la protezione dei diritti fondamentali, la sicurezza e l'affidabilità dei sistemi di
intelligenza artificiale. Essa introduce requisiti specifici per i diversi livelli di rischio associati all'uso dell'IA
e stabilisce regole per le applicazioni ad alto rischio, come i sistemi di IA utilizzati nei settori della salute, dei
trasporti e della sicurezza.


2022
Il 25 novembre 2022, il Consiglio dell'Unione Europea ha reso pubblico l'orientamento generale[104] e le
modifiche apportate alla proposta di regolamento sull'intelligenza artificiale del 2021, noto come AI ACT.
Questo documento rappresenta un importante passo nel processo legislativo dell'UE per stabilire regole
armonizzate sull'intelligenza artificiale.

Tra le modifiche strutturali più significative, vi è una revisione della definizione di intelligenza artificiale,
che si limita ora ai sistemi sviluppati mediante approcci di apprendimento automatico e basati sulla logica e
sulla conoscenza. Inoltre, il divieto di utilizzo dei sistemi di intelligenza artificiale per pratiche di social
scoring viene esteso anche agli attori privati, non solo a quelli pubblici.

Il documento introduce nuovi criteri di classificazione e requisiti aggiuntivi per la categoria dei sistemi di
intelligenza artificiale ad alto rischio. Inoltre, viene introdotto un nuovo titolo dedicato ai sistemi di
intelligenza artificiale per finalità generali, che sono utilizzati per scopi multipli e diversi.

Un'altra modifica significativa riguarda la maggiore garanzia del principio di trasparenza nell'uso
dell'intelligenza artificiale. Questo sottolinea l'importanza di rendere i processi decisionali dei sistemi di
intelligenza artificiale comprensibili e accessibili agli utenti.

Infine, il documento prevede l'introduzione di misure a sostegno dell'innovazione nell'ambito
dell'intelligenza artificiale, riconoscendo la necessità di promuovere lo sviluppo e l'adozione di tecnologie
avanzate nel settore.


2023
Durante la riunione[105] congiunta delle commissioni per il Mercato interno e la protezione dei consumatori
(Imco) e per le Libertà civili, la giustizia e gli affari interni (Libe) del Parlamento europeo, tenutasi l'11
maggio, sono state approvate tutte le proposte e i compromessi separati riguardanti l'Artificial Intelligence
Act.
La relazione presentata dai co-relatori Brando Benifei (S&D) e Dragoş Tudorache (Renew Europe) sarà ora
sottoposta a votazione durante la prossima sessione plenaria dell'Eurocamera, prevista tra il 12 e il 15
giugno. Questo passo è fondamentale per avviare i negoziati inter-istituzionali con il Consiglio dell'UE, che
ha già adottato la propria posizione lo scorso 6 dicembre. L'obiettivo è concludere la legislatura, entro la
primavera del 2024, con l'approvazione della prima legislazione globale e completa sull'intelligenza
artificiale, che regolamenterà un aspetto cruciale per la gestione della doppia transizione digitale e verde
dell'Unione Europea.

Gli eurodeputati hanno sostenuto l'approccio proposto dalla Commissione europea nella sua proposta
sull'Artificial Intelligence Act dell'aprile 2021 per regolamentare le applicazioni dell'intelligenza artificiale in
base alla scala di rischio. La proposta prevede quattro livelli di rischio: minimo (come videogiochi con
intelligenza artificiale e filtri anti-spam), limitato (come chatbot), alto (come l'assegnazione di punteggi a
esami scolastici e professionali, l'elaborazione di curriculum, la valutazione delle prove in tribunale e la
chirurgia assistita da robot) e inaccettabile (come tutto ciò che costituisce una "chiara minaccia per la
sicurezza, i mezzi di sussistenza e i diritti delle persone", ad esempio, l'assegnazione di un "punteggio
sociale" da parte dei governi). Per il primo livello non sono previsti interventi, mentre l'ultimo livello sarà
totalmente vietato.

Secondo la posizione del Parlamento europeo, che diventerà definitiva con la votazione in sessione plenaria,
saranno severamente vietati i sistemi di intelligenza artificiale che presentano un livello di rischio
inaccettabile per la sicurezza delle persone. Questo divieto si applicherà anche ai sistemi che utilizzano
tecniche subliminali o manipolative intenzionali, sfruttano le vulnerabilità delle persone o sono utilizzati per
il social scoring. L'elenco delle applicazioni con un livello di rischio inaccettabile includerà anche i sistemi
di identificazione biometrica remota "in tempo reale" in luoghi pubblici e successivamente (ad eccezione
delle forze dell'ordine per perseguire gravi reati e solo previa autorizzazione giudiziaria), i sistemi di
categorizzazione biometrica basati su caratteristiche sensibili (come sesso, etnia, cittadinanza, religione,
orientamento politico) provenienti dai social media o dalle telecamere a circuito chiuso per creare database
di riconoscimento facciale. Inoltre, includerà sistemi di polizia predittivi (basati su profili, localizzazione o
comportamenti criminali passati) e software di riconoscimento delle emozioni utilizzati anche nella gestione
delle frontiere, nei luoghi di lavoro e nelle istituzioni educative. Uno dei compromessi più rilevanti riguarda
il divieto permanente dell'uso di dettagli biometrici per riconoscere le persone in luoghi pubblici (come
impronte digitali, DNA, voce, andatura), che è stato approvato con 58 voti a favore, 36 contrari e 10
astenuti. Per quanto riguarda l'alto rischio, gli eurodeputati hanno ampliato la classificazione prevista dall'
Artificial Intelligence Act, includendo danni alla salute, alla sicurezza, ai diritti fondamentali[106] o
all'ambiente, nonché i sistemi di intelligenza artificiale che possono influenzare gli elettori durante le
campagne politiche e i sistemi di raccomandazione utilizzati dalle piattaforme dei social media (come
previsto dal Digital Services Act). Per l'uso generale dei sistemi di intelligenza artificiale, i fornitori
dovranno garantire una "solida protezione" dei diritti fondamentali, ridurre i rischi e rispettare i requisiti di
progettazione, registrandosi nel database dell'UE. Per quanto riguarda i modelli di fondazione generativa
come ChatGPT, gli eurodeputati richiedono il rispetto dei requisiti di trasparenza e la pubblicazione dei dati
protetti da copyright utilizzati per l'addestramento. Al fine di promuovere l'innovazione, sono state introdotte
esenzioni per le attività di ricerca e per i componenti dell'intelligenza artificiale forniti con licenze open-
source. L'Ufficio dell'UE per l'intelligenza artificiale, il cui ruolo è stato riformato dal Parlamento europeo,
sarà responsabile del monitoraggio generale dell'attuazione dell'Artificial Intelligence Act.


In Italia
Nel 2023, l'Autorità garante per la protezione dei dati personali ha approvato un regolamento che prevede
tre principi per le decisioni terapeutiche assunte da sistemi automatizzati: trasparenza dei processi
decisionali, supervisione umane delle decisioni automatizzate e non discriminazione algoritmica.[107]


G7
Il 30 ottobre 2023 i membri del G7 sottoscrivono undici principi guida per la progettazione, produzione e
implementazione di sistemi di intelligenza artificiale avanzati, oltre ad un codice di condotta volontario per
gli sviluppatori dell'intelligenza artificiale nel contesto del Processo di Hiroshima.

L'intesa riceve il plauso di Ursula von der Leyen che vi ritrova i principi della Direttiva AI, in via di
finalizzazione.[108]


Stati Uniti d'America

2023
Nel marzo 2023 Google ha proposto un'agenda digitale per la definizione di un'intelligenza responsabile,
che prevede fra l'altro il rispetto delle normative vigenti in tema di privacy e sicurezza informatica.[109]

Nel maggio 2023 il vice presidente di Microsoft Brad Smith ha chiesto di firmare un ordine esecutivo che
obblighi tutte le società informatiche statunitensi ad adottare gli standard del National Institute of Standards
and Technology (NIST), che è incaricato di analizzare i loro rapporti annuali. La regolamentazione prevede
l'obbligo di arrestare l'intelligenza artificiale nei casi di emergenza.[110]

Nel luglio 2023 l'amministrazione di Joe Biden e le maggiori aziende informatiche del settore (tra cui
Alphabet, Microsoft, Meta, Anthropic e OpenAI) raggiungono un accordo che prevede una serie di regole
alle quali single operatori sono liberi da aderire su base volontaria, in attesa dell'approvazione di una
regolamentazione vincolante da parte del Congresso USA.[111]

Uno dei primi punti che le aziende di settore si sono dette pronte a introdurre è quello relativo ai watermark
che dovrebbero identificare inequivocabilmente testo, video, audio e immagini generate dall'IA.[112]

Nel luglio 2023 Anthropic, Google, Microsoft e OpenAI hanno dato vita al Frontier Model Forum. A
partire dal 2024 lo sviluppo della Intelligenza artificiale è stato indirizzato alla mitigazione e all’adattamento
ai cambiamenti climatici, alla diagnosi precoce e la prevenzione del cancro.[113] Secondo il presidente di
Microsoft Brad Smith, le aziende produttrici di modelli di frontiera hanno il compito di garantire che questa
tecnologia "sia sicura, protetta e rimanga sotto il controllo umano".[114]

Il 30 ottobre 2023 il presidente Biden firma un ordine esecutivo che obbliga gli sviluppatori dell'intelligenza
artificiale a condividere i propri test di sicurezza con il governo prima di rendere pubblici i loro
software.[115] Il National Institute of Standards and Technology stabilisce lo standard per lo svolgimento di
questi test di sicurezza.[116] il dipartimento per il commercio sviluppa le linee guida e la filigrana per
l'autenticazione dei contenuti generati con l'intelligenza artificiale. L'AI trova impiego "nelle sentenze, nella
libertà condizionale e nella libertà vigilata, nella scarcerazione e nella detenzione preventiva, nelle
valutazioni dei rischi, nella sorveglianza, nella previsione del crimine e nella polizia predittiva e nell’analisi
forense".[116]


Svizzera
Nel 2023 l'incaricato federale della protezione dei dati e della trasparenza (IFPDT) ha ribadito in forma
scritta che la legge sulla protezione dei dati si applica anche all'intelligenza artificiale.[117]


Chiesa cattolica
Il 1° gennaio 2024, in occasione della 57a Giornata mondiale della pace, papa Francesco ha divulgato il
documento dal titolo Intelligenza artificiale e pace nel quale propone la sottoscrizione di un trattato
internazionale vincolante per lo sviluppo e l'uso dell'intelligenza artificiale anche a favore dei Paesi
emarginati; l'istituzione e rafforzamento di organismi internazionali che si occupino delle questioni etiche e
dell'intelligenza artificiale e dalla tutela dei diritti dei cittadini; l'elaborazione dell'algoretica, vale a dire di
un'etica degli algoritmi, nelle fasi di sperimentazione, progettazione, produzione, distribuzione e
commercializzazione dell'intelligenza artificiale; la tutela della sicurezza dell'occupazione dell'equità dei
salari rispetto a rischio di robotizzazione e automazione industriale del lavoro.[118]

In occasione della 58ª Giornata mondiale della Comunicazioni sociali, è stato divulgato un nuovo
documento dal titolo Intelligenza artificiale e sapienza del cuore: per una comunicazione pienamente
umana. Esso afferma, in accordo con Edmund Husserl, che la tecnologia non è solo un insieme di
strumenti, ma un quadro (Gestell) che incorpora e promuove un sistema di valori e di credenze
implicito.[119] Ad esempio, uno strumento di IA di un’agenzia di reclutamento potrebbe filtrare i candidati i
cui modelli di discorso non si conformano a ciò che l’IA considera educato. Occorre quindi esplicitare
questi valori della tecnologia ed armonizzarli con quelli della specie umana.


Dibattito filosofico
     Lo stesso argomento in dettaglio: Funzionalismo (filosofia della mente) e Qualia.

Rispondere alla domanda “Può una macchina pensare?” "la macchina è inanimata?" è dibattito tuttora
aperto a causa di argomentazioni a favore (Daniel Dennett, Hilary Putnam, Roger Penrose) e contro
(Hubert Dreyfus, John Searle, Gerald Edelman, Jerry Fodor).

Esistono due correnti filosofiche diverse che cercano di definire una macchina intelligente come prodotto
della:
- Intelligenza artificiale debole (weak AI): alla base di questo pensiero sta la convinzione che una macchina possa essere programmata con delle regole ben definite, in modo da comportarsi in modo intelligente.
- Intelligenza artificiale forte (strong AI): alla base di questo pensiero sta il fatto che una macchina agisca in modo intelligente implica che essa sia anche cosciente di come realmente si comporta.
Nel 1950 Alan Turing, nel suo articolo “Computing Machinery and Intelligence”[120], porta il dibattito filosofico a un livello più pragmatico, dando una definizione operativa di intelligenza basata su un test comportamentale inventato da lui stesso, chiamato "The Imitation Game" e ricordato anche come "Test di
Turing".

Il test si basa sull'esistenza di tre stanze allineate in cui nella prima c'è un uomo e nell'ultima una donna; in quella centrale invece risiede l'interrogante. L'uomo e la donna possono comunicare messaggi di testo solamente con l'interrogatore scrivendo tramite una tastiera e leggendo tramite uno schermo. L'obiettivo della donna è quello di farsi identificare come donna, mentre quello dell'uomo è quello di trarre in inganno l'interrogante, facendogli credere di essere una donna. Il gioco è ripetuto una seconda volta, scambiando l'uomo con una macchina.

La macchina è definita come intelligente se la frequenza con cui l'interrogante individua correttamente
l'uomo e la donna è almeno la stessa con cui individua correttamente la macchina e la donna.

Una macchina può quindi ritenersi intelligente se e solo se si comporta come un essere umano, quindi solo
se riesce a ingannare l'interrogante come farebbe un uomo.

In seguito, John Searle descrive nell'articolo "Minds, Brains and Programs"[121] un esperimento mentale
contro l'intelligenza artificiale forte, chiamato “la stanza cinese”. Egli vuole dimostrare che una macchina in
grado di superare il test di Turing, non è capace di capire cosa succede al suo interno; non è, quindi,
cosciente di come agisce. L'esperimento consiste in una persona che conosce solo l’inglese, munita di un
libro di grammatica cinese scritto in inglese e vari fogli, alcuni bianchi e alcuni con dei simboli. La persona
è dentro alla stanza con una piccola finestra verso l'esterno. Attraverso la finestra appaiono simboli
indecifrabili. La persona trova delle corrispondenze con i simboli del libro delle regole e segue le istruzioni.
Le istruzioni possono includere scrivere simboli su un nuovo foglio, trovare nuovi simboli, ecc. Infine,
questi fogli scritti verranno passati al mondo esterno, attraverso la finestra. Per un osservatore esterno, la
macchina sta ricevendo simboli cinesi, li sta elaborando e sta rispondendo con altri simboli, esattamente
come farebbe un uomo cosciente. In questo senso, secondo il test di Turing dovrebbe essere ritenuta
intelligente. Il problema, che sottolinea Searle, è che in realtà al suo interno, niente della macchina conosce
effettivamente il cinese, per cui non è cosciente di quello che sta effettivamente facendo. Secondo Searle
essa sta semplicemente seguendo un insieme di regole descritte nel libro. Secondo Daniel Dennett il
dibattito rimane però aperto in quanto Searle non riesce a dimostrare pragmaticamente la sua tesi, dovendo
far così ricorso alla intuizione.
